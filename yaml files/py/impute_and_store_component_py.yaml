name: Impute and store
inputs:
- {name: pre_impute_dataset, type: Dataset}
- {name: bucket_name, type: String}
outputs:
- {name: post_impute_dataset, type: Dataset}
- {name: cat_imputer, type: Model}
- {name: num_imputer, type: Model}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'sklearn' 'numpy' 'google.cloud' 'kfp==1.8.12' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef impute_and_store(pre_impute_dataset: Input[Dataset],\n   \
      \                    post_impute_dataset: Output[Dataset],\n               \
      \        cat_imputer: Output[Model],\n                       num_imputer: Output[Model],\n\
      \                       bucket_name: str,):\n\n    from sklearn.impute import\
      \ SimpleImputer\n    import pandas as pd\n    import numpy as np\n    import\
      \ pickle\n    from google.cloud import storage\n\n  #  import cloudstorage as\
      \ gcs\n\n    def recode_TOT_OB_CALL_INTL_ROAM_CNT(data):\n        if data <=\
      \ 100:\n            return(\"<=100\")\n        elif data <= 200:\n         \
      \   return(\"<=200\")\n        else:\n            return(\">200\")\n\n    def\
      \ recode_education(data):\n        if data == \" .\":\n            return(\"\
      none\")\n        elif int(data) >= 3:\n            return(\">=3\")\n       \
      \ else:\n            return(data)\n\n    NUMERICAL_FEATURE_NAMES = [\n     \
      \   \"SUBS_TENURE\",\n        \"TOT_DAY_LAST_COMPLAINT_CNT\",\n        \"TOT_DAY_LAST_SUSPENDED_CNT\"\
      ,\n        \"MTH_TO_SUBS_END_CNT\",\n        'REV_AMT_BASE_1',\n        'REV_AMT_BASE_2',\n\
      \        'CUST_AGE',\n        'PCT_CHNG_IB_SMS_CNT'\n    ]\n\n    EMBEDDING_CATEGORICAL_FEATURES\
      \ = [\n        \"GENDER_CD\",\n        \"EDUCATION_CD\",\n        \"TOT_SRV_DROPPED_CNT\"\
      ,\n        \"TOT_OB_CALL_INTL_ROAM_CNT\",\n        'BARRING_REASON_CD', \n \
      \       \"TOT_SRV_ADDED_CNT\"]\n\n    TARGET_LABEL = ['CHURN_FLG']\n\n    pre_data\
      \ = pd.read_csv(pre_impute_dataset.path + '.csv')\n    pre_data[\"EDUCATION_CD\"\
      ] = pre_data[\"EDUCATION_CD\"].apply(recode_education)\n    pre_data[\"TOT_OB_CALL_INTL_ROAM_CNT\"\
      ] = pre_data[\"TOT_OB_CALL_INTL_ROAM_CNT\"].apply(recode_TOT_OB_CALL_INTL_ROAM_CNT)\n\
      \    pre_data_cat = pre_data[EMBEDDING_CATEGORICAL_FEATURES]\n    pre_data_num\
      \ = pre_data[NUMERICAL_FEATURE_NAMES]\n\n    bucket = storage.Client().bucket(bucket_name)\n\
      \n    cat_imp = SimpleImputer(strategy = 'most_frequent')\n    num_imp = SimpleImputer(strategy\
      \ = 'median')\n    imputed_data_cat = cat_imp.fit_transform(pre_data_cat)\n\
      \    imputed_data_num = num_imp.fit_transform(pre_data_num)\n\n    # Store Cat\
      \ Imputer\n    file_name = cat_imputer.path + f\".pkl\"\n    with open(file_name,\
      \ 'wb') as file:  \n        pickle.dump(cat_imp, file)\n    cat_imp_name = \"\
      cat_imputer\" + f\".pkl\"\n    with open(cat_imp_name, 'wb') as file:  \n  \
      \      pickle.dump(cat_imp, file)\n\n    blob = bucket.blob('{}/{}'.format(\"\
      churn/artifact/preprocess\",\n                                    cat_imp_name))\n\
      \    blob.upload_from_filename(cat_imp_name)\n\n    # Store Num Imputer\n  \
      \  file_name = num_imputer.path + f\".pkl\"\n    with open(file_name, 'wb')\
      \ as file:  \n        pickle.dump(num_imp, file)\n    num_imp_name = \"num_imputer\"\
      \ + f\".pkl\"\n    with open(num_imp_name, 'wb') as file:  \n        pickle.dump(num_imp,\
      \ file)\n\n    blob = bucket.blob('{}/{}'.format(\"churn/artifact/preprocess\"\
      ,\n                                    num_imp_name))\n    blob.upload_from_filename(num_imp_name)\
      \  \n\n    out_df = pd.DataFrame(np.concatenate((imputed_data_cat, imputed_data_num,\
      \ pre_data[TARGET_LABEL].values),axis = 1),\n                        columns\
      \ = EMBEDDING_CATEGORICAL_FEATURES + NUMERICAL_FEATURE_NAMES + TARGET_LABEL)\n\
      \    out_df.to_csv(post_impute_dataset.path + \".csv\" , index=False, encoding='utf-8-sig')\n\
      \n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - impute_and_store
