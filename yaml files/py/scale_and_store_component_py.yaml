name: Encode scale and store
inputs:
- {name: pre_enc_dataset, type: Dataset}
- {name: bucket_name, type: String}
outputs:
- {name: post_enc_dataset, type: Dataset}
- {name: encoder, type: Model}
- {name: scaler, type: Model}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'sklearn' 'numpy' 'google.cloud' 'kfp==1.8.12' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef encode_scale_and_store(pre_enc_dataset: Input[Dataset],\n\
      \                     post_enc_dataset: Output[Dataset],\n                 \
      \    encoder: Output[Model],\n                     scaler: Output[Model],\n\
      \                     bucket_name: str):\n\n    import pandas as pd\n    import\
      \ numpy as np\n    import re\n    import pickle\n    from google.cloud import\
      \ storage\n    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n\
      \n    NUMERICAL_FEATURE_NAMES = [\n        \"SUBS_TENURE\",\n        \"TOT_DAY_LAST_COMPLAINT_CNT\"\
      ,\n        \"TOT_DAY_LAST_SUSPENDED_CNT\",\n        \"MTH_TO_SUBS_END_CNT\"\
      ,\n        'REV_AMT_BASE_1',\n        'REV_AMT_BASE_2',\n        'CUST_AGE',\n\
      \        'PCT_CHNG_IB_SMS_CNT']\n\n    EMBEDDING_CATEGORICAL_FEATURES ={\n \
      \       \"x0_\" :\"GENDER_CD\",\n        \"x1_\" :\"EDUCATION_CD\",\n      \
      \  \"x2_\" :\"TOT_SRV_DROPPED_CNT\",\n        \"x3_\" :\"TOT_OB_CALL_INTL_ROAM_CNT\"\
      ,\n        \"x4_\" :'BARRING_REASON_CD', \n        \"x5_\" :\"TOT_SRV_ADDED_CNT\"\
      }\n\n    TARGET_LABEL = ['CHURN_FLG']\n\n    pre_data = pd.read_csv(pre_enc_dataset.path\
      \ + '.csv')\n    pre_data_cat = pre_data[EMBEDDING_CATEGORICAL_FEATURES.values()]\n\
      \    pre_data_num = pre_data[NUMERICAL_FEATURE_NAMES]\n\n    bucket = storage.Client().bucket(bucket_name)\n\
      \n    enc = OneHotEncoder()\n    scl = StandardScaler()\n\n    enc_data = enc.fit_transform(pre_data_cat)\n\
      \    enc_dict = {v:[] for v in EMBEDDING_CATEGORICAL_FEATURES.values()}\n  \
      \  for feature in enc.get_feature_names_out():\n        for key in enc_dict:\n\
      \            if key in feature:\n                enc_dict[key].append(feature)\n\
      \n    file_name = encoder.path + f\".pkl\"\n    with open(file_name, 'wb') as\
      \ file:  \n        pickle.dump(enc, file)\n    enc_file_name = 'encoder' + f'.pkl'\n\
      \    with open(enc_file_name, 'wb') as file:  \n        pickle.dump(enc, file)\n\
      \n    blob = bucket.blob('{}/{}'.format(\"churn/artifact/preprocess\", enc_file_name))\n\
      \    blob.upload_from_filename(enc_file_name)  \n\n    scl_data = scl.fit_transform(pre_data_num)\n\
      \    scl_dict = {}\n    for i in range(len(NUMERICAL_FEATURE_NAMES)):\n    \
      \    scl_dict[NUMERICAL_FEATURE_NAMES[i]] = {'mean': scl.mean_[i], 'std':scl.scale_[i]}\n\
      \    file_name = scaler.path + f\".pkl\"\n    with open(file_name, 'wb') as\
      \ file:  \n        pickle.dump(scl, file)\n    scl_file_name = 'scaler' + f'.pkl'\n\
      \    with open(scl_file_name, 'wb') as file:  \n        pickle.dump(scl, file)\n\
      \n    blob = bucket.blob('{}/{}'.format(\"churn/artifact/preprocess\",\n   \
      \                                 scl_file_name))   \n    blob.upload_from_filename(scl_file_name)\
      \ \n\n    column_labels = list(enc.get_feature_names_out()) + NUMERICAL_FEATURE_NAMES\
      \ + TARGET_LABEL\n    out_df = pd.DataFrame(np.concatenate((enc_data.toarray(),\
      \ scl_data, pre_data[TARGET_LABEL].values),axis = 1),\n                    \
      \      columns = column_labels)\n    out_df.to_csv(post_enc_dataset.path + \"\
      .csv\" , index=False, encoding='utf-8-sig')\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - encode_scale_and_store
