{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmiTZpZTFTxw"
      },
      "outputs": [],
      "source": [
        "USER_FLAG = '--user'\n",
        "!pip install -U pip\n",
        "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.3.0 --upgrade\n",
        "!pip3 install {USER_FLAG} kfp --upgrade\n",
        "!pip install google_cloud_pipeline_components\n",
        "! python -m pip install --user virtualenv\n",
        "! echo \"create env\"\n",
        "! python -m venv vertex_venv\n",
        "! echo \"Add kernel to jupyter\"\n",
        "! ipython kernel install --name \"vertex_env\" --user "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VDTWjZTvFaUU"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b-TDwbglC3st",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d346d715-b9f4-44e4-c8ab-5bb740dfa655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Operation \"operations/acat.p2-352395168404-16137358-8a85-48e3-b54b-418a511d6acf\" finished successfully.\n",
            "env: PATH=/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/home/jupyter/.local/bin\n",
            "Project ID: churn-smu\n",
            "Pipeline Root: gs://practice-smu-123/churn/pipeline_root_churn\n",
            "GCS Bucket Name: practice-smu-123\n",
            "DataRoot Train Directory: gs://practice-smu-123/churn/data/dev\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = 'churn-smu'\n",
        "REGION = 'asia-southeast1'\n",
        "BUCKET_NAME = 'practice-smu-123'\n",
        "\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "! gcloud services enable  compute.googleapis.com \\\n",
        "                          containerregistry.googleapis.com \\\n",
        "                          aiplatform.googleapis.com \\\n",
        "                          cloudbuild.googleapis.com \\\n",
        "                          cloudfunctions.googleapis.com \\\n",
        "                          dataflow.googleapis.com\n",
        "\n",
        "PATH=%env PATH\n",
        "%env PATH = {PATH}:/home/jupyter/.local/bin\n",
        "\n",
        "PIPELINE_ROOT = \"gs://\" + f'{BUCKET_NAME}/churn/pipeline_root_churn'\n",
        "DATA_ROOT_TRAIN = \"gs://\" + f\"{BUCKET_NAME}/churn/data/dev\"\n",
        "DATA_ROOT_EVAL = \"gs://\" + f\"{BUCKET_NAME}/churn/data/val\"\n",
        "DATA_ROOT_SERVE = \"gs://\" + f\"{BUCKET_NAME}/churn/data/serve\"\n",
        "\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"Pipeline Root: {PIPELINE_ROOT}\")\n",
        "print(f\"GCS Bucket Name: {BUCKET_NAME}\")\n",
        "print(f\"DataRoot Train Directory: {DATA_ROOT_TRAIN}\")\n",
        "\n",
        "from typing import NamedTuple\n",
        "from kfp.v2 import dsl\n",
        "from kfp.v2.dsl import Artifact, Dataset, Input, Model, Output, Metrics, ClassificationMetrics, component, OutputPath, InputPath\n",
        "import kfp\n",
        "from kfp.v2 import compiler\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import pipeline_jobs\n",
        "from google_cloud_pipeline_components import aiplatform as gcc_aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5h6_pon18md"
      },
      "source": [
        "### Yaml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xQCkatArGjmU"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "GCR_IMAGE=\"gcr.io/churn-smu/churn-data-digestion:latest\"\n",
        "\n",
        "cat > data_ingest_component.yaml <<HERE\n",
        "\n",
        "name: data_ingest\n",
        "description: Download the dataset from GCS to pass to next component\n",
        "inputs:\n",
        "- {name: datapath, type: String}\n",
        "outputs:\n",
        "- {name: dataset, type: Dataset}\n",
        "implementation:\n",
        "  container:\n",
        "    image: $GCR_IMAGE\n",
        "    command:\n",
        "    - python\n",
        "    - data_ingest.py\n",
        "    args:\n",
        "    - --datapath\n",
        "    - {inputValue: datapath}\n",
        "    - --dataset\n",
        "    - {outputUri: dataset}\n",
        "HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bewfxHp0IXlo"
      },
      "outputs": [],
      "source": [
        "ingest = kfp.components.load_component_from_file(\"data_ingest_component.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XWh0IxYuZe1Z"
      },
      "outputs": [],
      "source": [
        "### YAML for data imputation\n",
        "\n",
        "%%bash\n",
        "\n",
        "GCR_IMAGE=\"gcr.io/churn-smu/churn-data-impute-store:latest\"\n",
        "\n",
        "cat > data_impute_component.yaml <<HERE\n",
        "\n",
        "name: data_impute\n",
        "description: Download the dataset from GCS to pass to next component\n",
        "inputs:\n",
        "- {name: pre_impute_dataset, type: Dataset}\n",
        "- {name: bucket, type: String}\n",
        "outputs:\n",
        "- {name: post_impute_dataset, type: Dataset}\n",
        "implementation:\n",
        "  container:\n",
        "    image: $GCR_IMAGE\n",
        "    command:\n",
        "    - python\n",
        "    - impute_and_store.py\n",
        "    args:\n",
        "    - --pre_impute_dataset\n",
        "    - {inputUri: pre_impute_dataset}\n",
        "    - --bucket\n",
        "    - {inputValue: bucket}\n",
        "    - --post_impute_dataset\n",
        "    - {outputUri: post_impute_dataset}\n",
        "HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sDLkMSy-Z58J"
      },
      "outputs": [],
      "source": [
        "impute = kfp.components.load_component_from_file(\"data_impute_component.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C70gJ2LcL10A"
      },
      "outputs": [],
      "source": [
        "### YAML for data scaling and encoding\n",
        "\n",
        "%%bash\n",
        "\n",
        "GCR_IMAGE=\"gcr.io/churn-smu/churn-data-enc-scl-store:latest\"\n",
        "\n",
        "cat > data_enc_scl_store_component.yaml <<HERE\n",
        "\n",
        "name: data_encoding_scaling_store\n",
        "description: Fit transform OneHotEncoder and StandardScaler and upload model artifacts to GCS\n",
        "inputs:\n",
        "- {name: pre_enc_dataset, type: Dataset}\n",
        "- {name: bucket_name, type: String}\n",
        "outputs:\n",
        "- {name: post_enc_dataset, type: Dataset}\n",
        "implementation:\n",
        "  container:\n",
        "    image: $GCR_IMAGE\n",
        "    command:\n",
        "    - python\n",
        "    - enc_scl_store.py\n",
        "    args:\n",
        "    - --pre_enc_dataset\n",
        "    - {inputUri: pre_enc_dataset}\n",
        "    - --bucket_name\n",
        "    - {inputValue: bucket_name}\n",
        "    - --post_enc_dataset\n",
        "    - {outputUri: post_enc_dataset}\n",
        "HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E95L3vCQMfVX"
      },
      "outputs": [],
      "source": [
        "enc_and_scl_store = kfp.components.load_component_from_file(\"data_enc_scl_store_component.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yHNoLitOPZpD"
      },
      "outputs": [],
      "source": [
        "### YAML for data hyperparameter tuning\n",
        "\n",
        "%%bash\n",
        "\n",
        "GCR_IMAGE=\"gcr.io/churn-smu/churn-data-hyperparameter_tuning:latest\"\n",
        "\n",
        "cat > hyperparameter_tuning_component.yaml <<HERE\n",
        "\n",
        "name: hyperparameter_tuning\n",
        "description: Perform Hyperparameter Tuning and Store Data inside GCS as json\n",
        "inputs:\n",
        "- {name: dataset, type: Dataset}\n",
        "- {name: bucket_name, type: String}\n",
        "implementation:\n",
        "  container:\n",
        "    image: $GCR_IMAGE\n",
        "    command:\n",
        "    - python\n",
        "    - hyperparameter_tuning.py\n",
        "    args:\n",
        "    - --dataset\n",
        "    - {inputUri: dataset}\n",
        "    - --bucket_name\n",
        "    - {inputValue: bucket_name}\n",
        "HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KYtAL9hNPZ5P"
      },
      "outputs": [],
      "source": [
        "hyperparameter_tuning = kfp.components.load_component_from_file(\"hyperparameter_tuning_component.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### YAML for data training\n",
        "\n",
        "%%bash\n",
        "\n",
        "GCR_IMAGE=\"gcr.io/churn-smu/churn-data-training:latest\"\n",
        "\n",
        "cat > training_component.yaml <<HERE\n",
        "\n",
        "name: training\n",
        "description: Perform training and store artifacts\n",
        "inputs:\n",
        "- {name: dataset, type: Dataset}\n",
        "- {name: bucket_name, type: String}\n",
        "outputs:\n",
        "- {name: model, type: Model}\n",
        "- {name: classification_metrics, type: ClassificationMetrics}\n",
        "- {name: base_metrics, type: Metrics}\n",
        "- {name: feature_importance, type: Dataset}\n",
        "implementation:\n",
        "  container:\n",
        "    image: $GCR_IMAGE\n",
        "    command:\n",
        "    - python\n",
        "    - training.py\n",
        "    args:\n",
        "    - --dataset\n",
        "    - {inputUri: dataset}\n",
        "    - --bucket_name\n",
        "    - {inputValue: bucket_name}\n",
        "    - --model\n",
        "    - {outputUri: model}\n",
        "    - --classification_metrics\n",
        "    - {outputUri: classification_metrics}\n",
        "    - --base_metrics\n",
        "    - {outputUri: base_metrics}\n",
        "    - --feature_importance\n",
        "    - {outputUri: feature_importance}\n",
        "HERE"
      ],
      "metadata": {
        "id": "7hPVrtR2WYkV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = kfp.components.load_component_from_file(\"training_component.yaml\")"
      ],
      "metadata": {
        "id": "HOQFjydXWYx3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MuiY-nGcL7fS"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(pipeline_root = PIPELINE_ROOT, name = \"churn-test\")\n",
        "\n",
        "def pipeline(data_root_train: str = DATA_ROOT_TRAIN + '/dev.csv',\n",
        "             bucket_name: str = BUCKET_NAME,\n",
        "             project: str = PROJECT_ID,\n",
        "             region: str = REGION,\n",
        "             ):\n",
        "    \n",
        "    data_train_op = ingest(data_root_train)\n",
        "    impute_and_store_op = impute(data_train_op.outputs['dataset'],\n",
        "                                 bucket = bucket_name)\n",
        "    enc_and_scl_store_op = enc_and_scl_store(impute_and_store_op.outputs['post_impute_dataset'],\n",
        "                                             bucket_name = bucket_name)\n",
        "    hypertune_op = hyperparameter_tuning(enc_and_scl_store_op.outputs['post_enc_dataset'],\n",
        "                                         bucket_name = bucket_name)\n",
        "    # train_op = train(enc_and_scl_store_op.outputs['post_enc_dataset'],\n",
        "    #                  bucket_name = bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DNFkcp5QPRV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JxrQISbQQgt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDRapEwAQQ4l"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAJKTaxFQRbF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_9w24QXMg2c",
        "outputId": "d99b379e-3f7c-47f3-eb8d-0975f45e6994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating PipelineJob\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
            "  category=FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineJob created. Resource name: projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659\n",
            "To use this PipelineJob in another session:\n",
            "pipeline_job = aiplatform.PipelineJob.get('projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659')\n",
            "View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/asia-southeast1/pipelines/runs/churn-test-20220702082659?project=352395168404\n",
            "PipelineJob projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob run completed. Resource name: projects/352395168404/locations/asia-southeast1/pipelineJobs/churn-test-20220702082659\n"
          ]
        }
      ],
      "source": [
        "compiler.Compiler().compile(pipeline_func = pipeline,\n",
        "                            package_path = 'churn_test.json')\n",
        "start_pipeline = pipeline_jobs.PipelineJob(display_name = 'churn-test',\n",
        "                                           template_path = 'churn_test.json',\n",
        "                                           enable_caching = False,\n",
        "                                           location = REGION,\n",
        "                                           project = PROJECT_ID,\n",
        "                                           )\n",
        "start_pipeline.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.google.client import AIPlatformClient\n",
        "\n",
        "api_client = AIPlatformClient(\n",
        "                project_id=PROJECT_ID,\n",
        "                region=REGION,\n",
        "                )\n",
        "\n",
        "SERVICE_ACCOUNT = (\n",
        "    \"352395168404-compute@developer.gserviceaccount.com\" # Replace the Xs with your generated service-account.\n",
        ")\n",
        "response = api_client.create_schedule_from_job_spec(\n",
        "    enable_caching=True,\n",
        "    job_spec_path=\"churn_test.json\",\n",
        "    schedule=\"0 0 4 * *\",\n",
        "    time_zone=\"Asia/Singapore\",  # change this as necessary\n",
        "  #  parameter_values={\"display_name\": 'test1'},\n",
        "    pipeline_root=PIPELINE_ROOT,  \n",
        "    service_account=SERVICE_ACCOUNT,    \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlDLFUwaBVwM",
        "outputId": "ff0adb24-8215-481f-ff36-925d4408f4fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
            "  category=FutureWarning,\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "container_version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}